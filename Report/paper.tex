% Version  Date        Author            Notes
% 1        ?           Tobias Lutz
% 2        April 2016  Tasnad Kernetzky  Updated to new IEEEtran class (v. 1.8)
%
%
\documentclass[journal, a4paper]{IEEEtran}

% check if we are running lua(la)tex and load font related packages appropriately
\usepackage{ifluatex}
\ifluatex
    \usepackage{fontspec}
\else
    \usepackage[T1]{fontenc}
    \usepackage[utf8]{inputenc}
\fi

% some very useful LaTeX packages include:
%\usepackage{cite}      % Written by Donald Arseneau
                        % V1.6 and later of IEEEtran pre-defines the format
                        % of the cite.sty package \cite{} output to follow
                        % that of IEEE. Loading the cite package will
                        % result in citation numbers being automatically
                        % sorted and properly "ranged". i.e.,
                        % [1], [9], [2], [7], [5], [6]
                        % (without using cite.sty)
                        % will become:
                        % [1], [2], [5]--[7], [9] (using cite.sty)
                        % cite.sty's \cite will automatically add leading
                        % space, if needed. Use cite.sty's noadjust option
                        % (cite.sty V3.8 and later) if you want to turn this
                        % off. cite.sty is already installed on most LaTeX
                        % systems. The latest version can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/cite/

\usepackage{graphicx}   % Written by David Carlisle and Sebastian Rahtz
                        % Required if you want graphics, photos, etc.
                        % graphicx.sty is already installed on most LaTeX
                        % systems. The latest version and documentation can
                        % be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/required/graphics/
                        % Another good source of documentation is "Using
                        % Imported Graphics in LaTeX2e" by Keith Reckdahl
                        % which can be found as esplatex.ps and epslatex.pdf
                        % at: http://www.ctan.org/tex-archive/info/

%\usepackage{psfrag}    % Written by Craig Barratt, Michael C. Grant,
                        % and David Carlisle
                        % This package allows you to substitute LaTeX
                        % commands for text in imported EPS graphic files.
                        % In this way, LaTeX symbols can be placed into
                        % graphics that have been generated by other
                        % applications. You must use latex->dvips->ps2pdf
                        % workflow (not direct pdf output from pdflatex) if
                        % you wish to use this capability because it works
                        % via some PostScript tricks. Alternatively, the
                        % graphics could be processed as separate files via
                        % psfrag and dvips, then converted to PDF for
                        % inclusion in the main file which uses pdflatex.
                        % Docs are in "The PSfrag System" by Michael C. Grant
                        % and David Carlisle. There is also some information
                        % about using psfrag in "Using Imported Graphics in
                        % LaTeX2e" by Keith Reckdahl which documents the
                        % graphicx package (see above). The psfrag package
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/psfrag/

%\usepackage{subfigure} % Written by Steven Douglas Cochran
                        % This package makes it easy to put subfigures
                        % in your figures. i.e., "figure 1a and 1b"
                        % Docs are in "Using Imported Graphics in LaTeX2e"
                        % by Keith Reckdahl which also documents the graphicx
                        % package (see above). subfigure.sty is already
                        % installed on most LaTeX systems. The latest version
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/subfigure/

%\usepackage{url}       % Written by Donald Arseneau
                        % Provides better support for handling and breaking
                        % URLs. url.sty is already installed on most LaTeX
                        % systems. The latest version can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/other/misc/
                        % Read the url.sty source comments for usage information.

%\usepackage{stfloats}  % Written by Sigitas Tolusis
                        % Gives LaTeX2e the ability to do double column
                        % floats at the bottom of the page as well as the top.
                        % (e.g., "\begin{figure*}[!b]" is not normally
                        % possible in LaTeX2e). This is an invasive package
                        % which rewrites many portions of the LaTeX2e output
                        % routines. It may not work with other packages that
                        % modify the LaTeX2e output routine and/or with other
                        % versions of LaTeX. The latest version and
                        % documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/sttools/
                        % Documentation is contained in the stfloats.sty
                        % comments as well as in the presfull.pdf file.
                        % Do not use the stfloats baselinefloat ability as
                        % IEEE does not allow \baselineskip to stretch.
                        % Authors submitting work to the IEEE should note
                        % that IEEE rarely uses double column equations and
                        % that authors should try to avoid such use.
                        % Do not be tempted to use the cuted.sty or
                        % midfloat.sty package (by the same author) as IEEE
                        % does not format its papers in such ways.

\usepackage{amsmath}   % From the American Mathematical Society
                        % A popular package that provides many helpful commands
                        % for dealing with mathematics. Note that the AMSmath
                        % package sets \interdisplaylinepenalty to 10000 thus
                        % preventing page breaks from occurring within multiline
                        % equations. Use:
\interdisplaylinepenalty=2500
                        % after loading amsmath to restore such page breaks
                        % as IEEEtran.cls normally does. amsmath.sty is already
                        % installed on most LaTeX systems. The latest version
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/



% Other popular packages for formatting tables and equations include:

%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty which improves the
% LaTeX2e array and tabular environments to provide better appearances and
% additional user controls. array.sty is already installed on most systems.
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/

% V1.6 of IEEEtran contains the IEEEeqnarray family of commands that can
% be used to generate multiline equations as well as matrices, tables, etc.

% Also of notable interest:
% Scott Pakin's eqparbox package for creating (automatically sized) equal
% width boxes. Available:
% http://www.ctan.org/tex-archive/macros/latex/contrib/supported/eqparbox/

% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.

\usepackage{algorithm}
\usepackage{algpseudocode} 
% Your document starts here!
\begin{document}

% Define document title and author
\title{{Report on}\\ \Large LeadFL: Client Self-Defense against Model Poisoning in Federated Learning}
\author{\textbf{Claus Guthmann}, \textbf{Ramazan Tan}}

\markboth{Trustworthy Distributed Learning }{}
\maketitle

%
% LeadFL - 4 Pages
%

% Write abstract here(50-80 words)
\begin{abstract}
    Existing defense mechanisms in federated learning often assume an approximately constant proportion of 
    malicious clients throughout training. However, when only a subset of clients participates in each 
    communication round, the fraction of malicious clients can vary substantially across rounds. 
    Over a sufficient number of iterations, this variability makes it likely that some rounds will 
    contain a majority of malicious participants. Under this premise, the paper \textit{LeadFL: Client Self-Defense against Model Poisoning in Federated Learning} \cite{LeadFL} demonstrates that poisoning attacks occurring in such rounds can have long-lasting effects on the global model. To address this issue, the authors propose a client-side defense mechanism that significantly improves the model’s ability to recover from these transient but severe poisoning events.

\end{abstract}

\section{Introduction}
\IEEEPARstart{f}{ederated} 
 learning enables the collaborative training of machine learning models across distributed clients while keeping local data private. However, this decentralized setting also exposes the training process to model poisoning attacks, where malicious clients manipulate their updates to degrade the global model. Many existing defense mechanisms implicitly assume that the proportion of malicious clients remains approximately constant throughout training. In practice, federated learning systems typically select only a subset of clients to participate in each communication round, causing the fraction of malicious clients to fluctuate over time. As a result, it becomes increasingly likely that some rounds contain an unusually high—or even majority—share of malicious participants, especially over long training horizons.

The paper \textit{LeadFL: Client Self-Defense against Model Poisoning in Federated Learning} \cite{LeadFL} addresses this often-overlooked scenario. It shows that poisoning attacks occurring during such adversarial spikes can have long-lasting effects on the global model, even when the system later returns to benign conditions. To mitigate this issue, LeadFL introduces a client-side defense mechanism designed to limit the impact of these transient but severe attacks and to accelerate recovery in subsequent rounds.

In this report, we begin by presenting the preliminary concepts and results required to understand the LeadFL approach. We then provide an overview of the method itself and discuss its potential advantages in federated learning environments subject to spiking adversarial participation, followed by an examination of its limitations. To further assess its effectiveness, we implemented the LeadFL algorithm and extended the original evaluation to cover previously unaddressed cases. Finally, we present the results of our own experiments, which corroborate that LeadFL significantly improves robustness against the attack patterns considered in the paper.

\section{Background}
A central idea is to reduce the lasting influence of a burstial attack. One way to formally characterize this lingering effect was introduced in \textit{FL-WBC: Enhancing Robustness against Model Poisoning Attacks in Federated Learning from a Client Perspective} \cite{FLwbc} through the notion of the \textbf{Attack Effect on Parameters}. For a given communication round $t$, let $W_t$ denote the global model parameters resulting from potentially malicious client updates, and let $W_t^M$ denote the parameters that would have been obtained in the absence of malicious behavior. The attack effect is then defined as
\begin{equation}
\delta_t := W_t - W_t^M.    
\end{equation}

The authors further derive a recursive approximation of this quantity,
\[
\delta_t \approx \hat{\delta}_t := \frac{1}{|C_t|} \left[ \sum_{c \in C_t} \prod_{i=0}^{I-1} \left( I - \eta_t H_{t,i}^c \right) \right] \hat{\delta}_{t-1},
\]
where $C_t$ denotes the set of clients selected in round $t$ (assumed to be equally weighted), $\eta_t$ is the learning rate, $I$ is the number of local training steps, and $H_{t,i}^c$ represents the Hessian matrix of the local loss function.

Empirically, the Hessian matrices $H_{t,i}^c$ are often highly sparse, which causes the aggregated transformation
\begin{equation}
\frac{1}{|C_t|} \left[ \sum_{c \in C_t} \prod_{i=0}^{I-1} \left( I - \eta_t H_{t,i}^c \right) \right]
\end{equation}
to remain close to the identity matrix. As a consequence, the attack effect $\delta_t$ decays only slowly over time, leading to the observed persistence of poisoning attacks.

The FL-WBC \cite{FLwbc} client-side defense seeks to counteract this  by injecting random noise into local updates, thereby accelerating the decay of the attack effect. Building on this insight, LeadFL \cite{LeadFL} proposes a more targeted approach: instead of relying on random perturbations, it introduces a specifically designed regularization term.

Directly computing the full Hessian matrix $H_{t,i}^c$ for  neural networks is, however, computationally prohibitive. To make the approach tractable in a federated learning setting, LeadFL relies on an efficient approximation of the Hessian inspired by the \textit{Optimal Brain Damage} framework \cite{OPD}.

Specifically, the method restricts attention to the diagonal elements of the Hessian, thereby ignoring inter-parameter dependencies. While this simplification sacrifices second-order interactions, it significantly reduces computational complexity and has been shown to provide useful curvature information in large-scale models. The diagonal entries capture the sensitivity of the gradient with respect to individual parameters and can be approximated via finite differences of successive gradients.

Formally, the Hessian matrix is approximated as
\begin{equation}
    \label{equation:diag}
    H_{t,i}^c \approx \tilde H_{t,i}^c := \operatorname{diag}\bigl(\nabla L(W_{t,i+1}^c) - \nabla L(W_{t,i}^c)\bigr),
\end{equation}
where $W_{t,i}^c$ denotes the local model parameters of client $c$ at global round $t$ and local step $i$, and $L(\cdot)$ is the local loss function. 

\section{LeadFL}
The core idea of LeadFL \cite{LeadFL} is to reduce the persistence of poisoning effects by locally minimizing the term $I - H_{t,i}^c$ on each client, thereby accelerating the decay of the attack effect described in the previous section. To achieve this, LeadFL augments the standard local optimization objective with an additional regularization term whose gradient is incorporated into each update step.

Concretely, each local update is computed in two stages. First, a standard gradient descent step is performed,
\begin{equation}
    \tilde W_{t,i+1}^c := W_{t,i}^c - \eta_t \nabla L(W_{t,i}^c),
\end{equation}
yielding intermediate model parameters $\tilde W_{t,i+1}^c$. In a second step, the regularization update is applied,
\begin{equation}
    W_{t,i+1}^c := \tilde W_{t,i+1}^c - \eta_t \alpha \, \operatorname{clip}\!\left( \nabla \bigl( I - \eta_t \tilde H_{t,i}^c \bigr), q \right),
\end{equation}
where $\alpha$ controls the strength of the regularization and \emph{clip} limits the absolute value of each entry to the clipping threshold $q$.

This two-stage formulation allows the intermediate parameters $\tilde W_{t,i+1}^c$ to be reused in constructing a computationally efficient approximation of the Hessian. However, the derivation of this approximation in the original paper contains errors. We therefore adopt the following formulation, which we believe to be a mathematically consistent approximation closely aligned with the intended method:
\begin{equation}
    \tilde H_{t,i}^c \approx (W_{t,i}^c - \tilde W_{t,i+1}^c + \Delta W_{t,i}^c)/\eta_t,
\end{equation}
where $\Delta W_{t,i}^c := W_{t,i}^c - W_{t,i-1}^c$ denotes the parameter change between successive local steps. It is important to note that this Hessian approximation is only accurate up to the contribution of the regularization term applied in the previous local update. 

Combining the previous steps, the local client iteration of LeadFL can be summarized as follows (cf. Algorithm 1 in \cite{LeadFL}):

\begin{algorithm}
\caption{\emph{LeadFL} Local Client Iteration}
\begin{enumerate}[]
    \item[1:] {Compute gradients and update intermediate weights:} \\
    $\tilde W_{t,i+1}^c \leftarrow W_{t,i}^c - \eta_t \nabla L(W_{t,i}^c)$
    
    \item[2:] {Estimate the Hessian matrix:} \\
    $\tilde H_{t,i}^c \leftarrow (W_{t,i}^c - \tilde W_{t,i+1}^c + \Delta W_{t,i}^c)/\eta_t$
    
    \item[3:] {Compute the regularization term:} \\
    $R_{t,i}^c \leftarrow \operatorname{clip}\!\left( \nabla \bigl(I - \eta_t \tilde H_{t,i}^c \bigr), q \right)$
    
    \item[4:] {Update the local weights with the regularization term:} \\
    $W_{t,i+1}^c \leftarrow \tilde W_{t,i+1}^c - \eta_t \alpha R_{t,i}^c$
\end{enumerate}
\end{algorithm}


The authors of LeadFL subsequently implement and evaluate the proposed defense under a range of experimental settings. Their primary setup consists of 100 clients, of which 25 are malicious. Training is performed over 80 global rounds, with each client executing 10 local training steps per round. In each global round, 10 clients are selected uniformly at random; this client selection is kept consistent across different experimental runs to ensure comparability (additional results for alternative client selection strategies are reported in the appendix of \cite{LeadFL}).

The evaluation considers several server-side aggregation defenses, namely SparseFed, Multi-Krum, and Bulyan. For the LeadFL client-side defense, the clipping threshold is fixed at $q = 0.2$. The regularization strength $\alpha$ is varied across datasets, with $\alpha = 0.4$ for FashionMNIST and $\alpha = 0.25$ for CIFAR10. Due to the randomized client selection process, the number of malicious participants fluctuates across rounds, leading to significant variability in backdoor success rates. To account for this effect, the authors report both the final backdoor accuracy and the average backdoor accuracy over the entire training process.

\begin{table}[!hbt]
    \begin{center}
    \caption{Comparison of defenses under 9-pixel pattern backdoor attack on IID FashionMNIST dataset. ( cf. Table 1 \cite{LeadFL})}
    \label{tab:IIDMNIST}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Server-side Defense & \multicolumn{4}{|c|}{Multi-Krum} \\\hline 
        Client-side Defense & None & LDP & FL-WBC & LeadFL \\ \hline 
        Maintask Accuracy  & 89.3 & 87 & 87.2 & 87.9  \\ \hline 
        Backdoor Accuracy Avg.  & 82.6 & 76.0 & 77.5 & 32.9 \\ \hline 
        Backdoor Accuracy Final  &93.2  &79.6  &80.6 &0.0 \\ \hline 
    \end{tabular}
    \end{center}
\end{table}
\begin{table}[!hbt]
    \begin{center}
    \caption{Comparison of defenses under 9-pixel pattern backdoor attack on IID CIFAR10 dataset. ( cf. Table 2 \cite{LeadFL})}
    \label{tab:IIDCIFAR}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Server-side Defense & \multicolumn{4}{|c|}{Multi-Krum} \\\hline 
        Client-side Defense & None & LDP & FL-WBC & LeadFL \\ \hline 
        Maintask Accuracy  & 76.3 &48.0 &43.3 &56.9  \\ \hline 
        Backdoor Accuracy Avg.  & 77.5 &53.1& 56.9 &35.6 \\ \hline 
        Backdoor Accuracy Final  &80.5 &43.8& 40.5 &25.6 \\ \hline 
    \end{tabular}
    \end{center}
\end{table}

Tables~\ref{tab:IIDMNIST} and~\ref{tab:IIDCIFAR} present a selection of results for the IID FashionMNIST and CIFAR10 datasets, respectively, under a 9-pixel pattern backdoor attack using the Multi-Krum server-side defense.

For the FashionMNIST dataset (Table~\ref{tab:IIDMNIST}), LeadFL consistently reduces the final backdoor accuracy to nearly zero while maintaining the main task accuracy within a 10\% reduction compared to the undefended baseline. In contrast, the alternative client-side defenses, LDP and FL-WBC, achieve only moderate reductions in backdoor accuracy.

For the more complex CIFAR10 dataset (Table~\ref{tab:IIDCIFAR}), the impact of LeadFL on the main task accuracy is more pronounced, reducing it to approximately 50--60\%. Nevertheless, LeadFL still substantially lowers the final backdoor accuracy to around 25\%, outperforming the other client-side defenses in terms of robustness.

Across both datasets and all evaluated server-side defenses, the absence of a client-side defense results in consistently high average backdoor accuracies—exceeding 75\%—highlighting the severity of bursty poisoning attacks and the necessity of additional client-side protection mechanisms.

\begin{figure}[!hbt]
    \begin{center}
        \caption{9-pixel back door attack on FashionMNIST when the attack is periodic(cf. Figure 3 \cite{LeadFL})}
    \label{fig:MNIST}
    \includegraphics[width=0.4\textwidth]{figs/MNIST-IID.png}
    \end{center}
\end{figure}

\begin{figure}[!hbt]
    \begin{center}
        \caption{9-pixel back door attack on CIFAR10 when the attack is periodic (cf. Figure 3 \cite{LeadFL})}
    \label{fig:CIFAR}
    \includegraphics[width=0.4\textwidth]{figs/CIFAR10IID.png}
    \end{center}
\end{figure}

The authors further investigate the behavior of LeadFL under explicitly bursty attack patterns. In this setting, client selection is no longer random. Instead, training proceeds in cycles of ten rounds, where the first two rounds contain six malicious clients out of ten selected participants, while the remaining eight rounds involve exclusively benign clients. 

The results of this experiment are illustrated in Figure~\ref{fig:MNIST} for the FashionMNIST dataset and in Figure~\ref{fig:CIFAR} for the CIFAR10 dataset. While LeadFL exhibits limited impact during the rounds dominated by malicious clients, its effect becomes apparent in the subsequent benign rounds. In particular, the recovery from each attack spike is significantly faster, and the achieved backdoor accuracy stabilizes at a lower level compared to the undefended baseline.

In summary, LeadFL introduces a novel client-side defense mechanism that is particularly effective at mitigating the long-lasting effects of spikes in malicious client participation. Although the method incurs a measurable reduction in main task accuracy, it demonstrates that client-side defenses can play a crucial role in complementing existing server-side aggregation strategies. These findings motivate a closer examination of LeadFL's practical limitations and empirical behavior, which we address in the remainder of this report through an extended experimental evaluation.


\section{Limitations and Extended Evaluation}
Before presenting our own experimental evaluation, we first discuss several limitations and ambiguities encountered in the original LeadFL paper. These issues are relevant both for the correct interpretation of the reported results and for the reproducibility of the proposed method.\\

First, while the regularization strength $\alpha$ plays a crucial role in balancing robustness against performance degradation, the paper does not provide a principled justification or systematic tuning strategy for the selected values. Instead, $\alpha$ is varied empirically across experiments without clear guidance, limiting the interpretability and generalizability of the reported outcomes. 


\begin{figure}[!hbt]
    \begin{center}
        \caption{9-pixel back door attack on FashionMNIST when the malicious client selection is periodic}
    \label{fig:FashionMNISTRegBA}
    \includegraphics[width=0.5\textwidth]{figs/MNIST-Reg-BA.png}
    \end{center}
\end{figure}

\begin{figure}[!hbt]
    \begin{center}
        \caption{9-pixel back door attack on FashionMNIST when the malicious client selection is periodic}
    \label{fig:FashionMNISTRegMA}
    \includegraphics[width=0.5\textwidth]{figs/MNIST-Reg-MA.png}
    \end{center}
\end{figure}

To investigate the sensitivity of LeadFL to the choice of $\alpha$, we conducted additional experiments using the authors’ original code with modified regularization strengths. Due to computational constraints, these experiments were limited to 40 global rounds. Figures~\ref{fig:FashionMNISTRegBA} and \ref{fig:FashionMNISTRegMA} show the resulting backdoor accuracy and main task accuracy, respectively. All of the evaluations in this section use the bulyan server-side defense.

As shown in Figure~\ref{fig:FashionMNISTRegBA}, reducing the regularization strength to $\alpha = 0.2$ significantly weakens the defense: the decline in backdoor accuracy is slower and can not reach the same lower bound compared to the default setting of $\alpha = 0.4$. Increasing the regularization strength to $\alpha = 0.6$ yields a slight improvement on average, but the effect is not consistent across all rounds.
Regarding the main task performance, illustrated in Figure~\ref{fig:FashionMNISTRegMA},  $\alpha = 0.2$ provides no observable benefit, while $\alpha = 0.6$ introduces occasional drops in accuracy.\\


Finally, as discussed in the previous sections, the Hessian approximation used in LeadFL appears to contain fundamental inconsistencies between the theoretical proposal and the reference implementation. 

The manuscript describes the defense mechanism as a direct regularization of the client-side Hessian matrix, explicitly deriving an approximation based on the finite difference of gradients. Under this formulation, the client is expected to perform a lookahead step to estimate the local curvature $\tilde{H}$ and essentially perturb the loss landscape to eliminate the null-space blind spots where the attack hides.

However, our analysis of the authors' official source code reveals a different approach. As detailed in \textbf{Algorithm 2}, the reference implementation does not compute the finite difference of gradients or estimate the Hessian diagonal. Rather than computing the Hessian approximation, the code employs a heuristic that minimizes the $L_2$ norm of the divergence between the current update and a theoretical ``unit step.'' This effectively constrains the weight trajectory through geometry rather than the curvature-based regularization proposed in the text. This raises the question of whether the method's reported efficacy stems from this norm-based clipping rather than the intended Hessian regularization.

\begin{algorithm}
\caption{\emph{LeadFL} Local Client Iteration (Original Implementation)}
\begin{enumerate}[]
    \item[1:] {Compute standard gradients:} \\
    $g_{t,i}^c \leftarrow \nabla L(W_{t,i}^c)$
    
    \item[2:] {Perform standard SGD step:} \\
    $\tilde W_{t,i+1}^c \leftarrow W_{t,i}^c - \eta_t g_{t,i}^c$
    
    \item[3:] {Compute regularization loss ($L_2$ Norm):} \\
    $L_{reg} \leftarrow \alpha \cdot \left\| \tilde W_{t,i+1}^c - W_{t,i}^c - g_{old}^c - \eta_t \cdot \mathbf{1} \right\|_2$
    
    \item[4:] {Compute gradients of the regularization loss:} \\
    $g_{reg}^c \leftarrow \nabla L_{reg}(\tilde W_{t,i+1}^c)$
    
    \item[5:] {Clip the regularization gradients:} \\
    $\tilde g_{reg}^c \leftarrow \operatorname{clip\_grad\_value}(g_{reg}^c, q)$
    
    \item[6:] {Secondary update step:} \\
    $W_{t,i+1}^c \leftarrow \tilde W_{t,i+1}^c - \eta_t \tilde g_{reg}^c$
\end{enumerate}
\end{algorithm}

To address this ambiguity and rigorously evaluate the paper's theoretical claims, we developed a corrected implementation, detailed in \textbf{Algorithm 3}. Unlike the original code, our version strictly adheres to the mathematical derivation provided in the manuscript, explicitly computing the gradient difference $(\tilde{g} - g) / \eta$ to approximate the Hessian diagonal. By testing both versions, we aim to determine if the defense's robustness relies on the intended Hessian regularization or merely on the norm constraint found in the code.

\begin{algorithm}
\caption{\emph{LeadFL} Local Client Iteration (Our Implementation)}
\begin{enumerate}[]
    \item[1:] {Compute gradients at current parameters:} \\
    $g_{t,i}^c \leftarrow \nabla L(W_{t,i}^c)$
    
    \item[2:] {Perform temporary SGD step (lookahead):} \\
    $\tilde W_{t,i+1}^c \leftarrow W_{t,i}^c - \eta_t g_{t,i}^c$
    
    \item[3:] {Compute gradients at the lookahead position:} \\
    $\tilde g_{t,i+1}^c \leftarrow \nabla L(\tilde W_{t,i+1}^c)$
    
    \item[4:] {Estimate the Hessian diagonal via Finite Difference:} \\
    $\tilde H_{t,i}^c \leftarrow (\tilde g_{t,i+1}^c - g_{t,i}^c) / \eta_t$
    
    \item[5:] {Compute and clip the regularization term:} \\
    $R_{raw} \leftarrow 1 - \eta_t \tilde H_{t,i}^c$ \\
    $R_{t,i}^c \leftarrow \operatorname{clamp}(R_{raw}, -q, q)$
    
    \item[6:] {Final update with regularization:} \\
    $W_{t,i+1}^c \leftarrow \tilde W_{t,i+1}^c - \eta_t \alpha R_{t,i}^c$
\end{enumerate}
\end{algorithm}

\begin{figure}[!hbt]
    \begin{center}
        \caption{9-pixel back door attack on FashionMNIST when the malicious client selection is periodic}
    \label{fig:FashionMNISTOwnMA}
    \includegraphics[width=0.5\textwidth]{figs/MNIST-Own-MA.png}
    \end{center}
\end{figure}
\begin{figure}[!hbt]
    \begin{center}
        \caption{9-pixel back door attack on FashionMNIST when tthe malicious client selection is periodic}
    \label{fig:FashionMNISTOwnBA}
    \includegraphics[width=0.5\textwidth]{figs/MNIST-Own-BA.png}
    \end{center}
\end{figure}

To evaluate this, we reproduced the periodic attack setup shown in Figure~\ref{fig:MNIST} and augmented it with results obtained from our own implementation. The corresponding main task accuracy and backdoor accuracy are shown in Figures~\ref{fig:FashionMNISTOwnMA} and~\ref{fig:FashionMNISTOwnBA}, respectively. We observe that the original implementation acts as a strict geometric constraint, whereas our version relies on a noisy finite-difference approximation. This distinction likely contributes to the performance variance, suggesting that the strict norm constraint in the reference code may offer additional stability distinct from the proposed Hessian regularization. Although our implementation performs slightly worse than the original results reported by the authors, the central qualitative behavior of LeadFL remains intact. In particular, the defense induces only a moderate reduction in main task accuracy while substantially improving the recovery rate following bursty poisoning attacks.


\begin{figure}[!hbt]
    \begin{center}
        \caption{9-pixel back door attack on FashionMNIST when the malicious client selection is random}
    \label{fig:FashionMNISTOwnMARandom}
    \includegraphics[width=0.5\textwidth]{figs/MNIST-Own-Random-MA.png}
    \end{center}
\end{figure}
\begin{figure}[!hbt]
    \begin{center}
        \caption{9-pixel back door attack on FashionMNIST when the malicious client selection is random}
    \label{fig:FashionMNISTOwnBARandom}
    \includegraphics[width=0.5\textwidth]{figs/MNIST-Own-Random-BA.png}
    \end{center}
\end{figure}

We also repeated the same evaluation using random client selection instead of the periodic pattern. Interestingly, the main-task accuracy, shown in Figure~\ref{fig:FashionMNISTOwnMARandom}, remains largely consistent with the periodic case. In contrast, the backdoor accuracy exhibits more pronounced differences. While LeadFL still provides the strongest defense toward the end of training, it is occasionally outperformed by other methods—including the baseline with no client-side defense—during the initial rounds.\\

In summary, our evaluation confirms that LeadFL is effective at its core: it accelerates recovery from bursty poisoning attacks and lowers the long-term backdoor accuracy, though at a cost to main-task performance. Crucially, the observation that our corrected Hessian-based implementation exhibits comparable qualitative behavior suggests that the defense’s efficacy is primarily driven by the theoretical regularization, though the implicit norm constraint in the original reference code appears to contribute additional stability. Notably, even with this trade-off, LeadFL maintains higher main-task accuracy compared to alternative client-side defenses. The periodic bursty attack pattern considered in this report represents an optimal scenario for LeadFL, whereas random client selection reduces its overall impact and introduces greater variability in backdoor mitigation. These findings suggest that while LeadFL shows promise, further evaluation is necessary before deployment in real-world federated learning systems. In particular, large-scale experiments and a more systematic investigation of the regularization parameter $\alpha$ in complex models would be essential.

% Now we need a bibliography:
\begin{thebibliography}{1}

    
    \bibitem{LeadFL} 
    C.~Zhu, S.~Roos, and L.~Chen. LeadFL: Client Self-Defense against Model Poisoning in Federated Learning. {\em Proceedings of the 40th International Conference on Machine Learning},
    vol.~202, pp.~43158--43180, Jul. 2023.
    
    \bibitem{FLwbc} 
    J.~Sun, A.~Li, L.~DiValentin, A.~Hassanzadeh, Y.~Chen and H.~Li. {FL-WBC:} Enhancing Robustness against Model Poisoning Attacks in
                  Federated Learning from a Client Perspective {\em Advances in Neural Information Processing Systems},
    vol.~34, pp.~12613--12624, Dec. 2021.

    \bibitem{OPD} 
    Y.~LeCun, J.~Denker and S.~Solla. Optimal Brain Damage {\em Advances in Neural Information Processing Systems},
    vol.~2, pp.~598-605, Nov. 1989.

\end{thebibliography}



\onecolumn
\newpage
\twocolumn


\section{Summary: Model Poisoning Attacks to Federated Learning via Multi-Round Consistency}

Many existing attacks on federated learning rely on assumptions that are often unrealistic in practice, such as knowledge of other clients' updates or awareness of the aggregation rule used by the server. The paper \emph{Model Poisoning Attacks to Federated Learning via Multi-Round Consistency} \cite{PoisonedFL} proposes an attack strategy that relaxes these assumptions and operates effectively without direct knowledge of other clients or the aggregation mechanism.\\

The principal idea behind the attack is to overcome the natural cancellation of updates caused by random noise added by benign clients. To achieve this, the malicious clients first select a consistent attack direction $s$. They then estimate the honest update by removing their own previous malicious contribution from the global model update.
\begin{equation}
    \tilde h = g^{t-1} - \frac{||g^{t-1}||}{||k^{t-1} \odot s||}(k^{t-1} \odot s)
\end{equation}
where $g^{t-1}$ is the last global model update and $k^{t-1} \odot s$ the last model updates on the fake clients. The factor $\frac{||g^{t-1}||}{||k^{t-1} \odot s||}$ normalizes the malicious update to have the same magnitude as the as the global one.
This estimate is normalized to form a template of a benign update.
\begin{equation}
    v^t = \frac{|\tilde{h}|}{||\tilde{h}||_2}
\end{equation}

\begin{equation}
    \lambda_t = c_t \dot ||g^{t-1}||_2
\end{equation}

A scaling factor $\lambda_t$ is determined relative to the norm of the global update, with an additional check: if the global model does not drift in the intended attack direction, it is assumed that the malicious updates are being filtered, and the scaling factor is reduced accordingly: $c_t = \begin{cases}
    c_{t-1}\qquad \text{Alignment }> 50\%\\
    \beta c_{t-1}\qquad \text{else }\\
\end{cases}$ with $\beta < 1$. The final malicious update $g_i^t$ is computed as the product of the normalized template, the scaling factor, and the attack direction.
\begin{equation}
    g_i^t = (v^t \cdot \lambda_t) \odot s
\end{equation}

The presenting group \cite{Presenters} conducted their own evaluation of this attack. They observed that the attack requires several rounds to become effective. Nevertheless, with at least 9\% of selected clients acting maliciously, the attack successfully degrades the global model's accuracy. 

The students further investigated the performance of a novel server-side defense, PRoDIGY, which was not included in the original paper. PRoDIGY proved extremely effective against this attack because the malicious updates are highly similar, making them easily detectable and filtered by the dissimilarity-based criteria. 

To examine potential circumvention strategies, the group adapted the attack to counter PRoDIGY. They tested two techniques proposed in the original paper: noise adaptation, where random perturbations are added to the malicious update, and sign flipping, where each element of the malicious direction vector is flipped with a small probability. For both techniques, different scaling factors and probabilities were evaluated. The noise adaptation strategy was over all more effective than sign flipping. However, in all cases, the main task accuracy remained above 80\%.

The group also noted that the original paper assumes a fixed, very small learning rate. They experimented with a learning rate scheduler, which accelerates the increase of main task accuracy and causes the learning rate to decay before the attack can fully take effect. Under this condition, the attack's effectiveness is further reduced.\\

 Building on the work of the group, we believe that an additional direction for future research is to investigate the impact of client selection on attack effectiveness. In the paper, the percentage of malicious clients is fixed after selection, remaining consistent across rounds. Since one of the main observations is that the attack's effect accumulates over time, it would be valuable to study the impact of a fully random client selection process, where the fraction of malicious participants varies from round to round.
 
 Further, if attackers are assumed to coordinate, an alternative strategy would be to partition the dimensions of the malicious direction vector among participants, with each attacker contributing only to their assigned subset of parameters. Against defenses such as PRoDIGY, which rely on detecting similarity across malicious updates, this approach could reduce the artificial correlation among adversarial updates and potentially increase the attack's effectiveness. 
\begin{thebibliography}{1}

    \bibitem{Presenters} 
    Y. Xie, M. Fang and N. Z. Gong. Model Poisoning Attacks to Federated Learning via Multi-Round Consistency. {\em  Trustworthy Distributed Learning, TUM},
    Jan. 2026.

    \bibitem{PoisonedFL} 
    L. Kunze, F. Durchdewald and N. Schenk. Presentation on: PoisonedFL via Multi-Round Consistency. {\em  Tri},
    vpp.~5454-15463, Jun. 2025.

\end{thebibliography}


\newpage
\section{Summary: Towards the Robustness of Differentially Private Federated Learning}

A common intuition in Federated Learning (FL) is that Differential Privacy (DP) noise naturally enhances robustness by "drowning out" malicious inputs. The paper \emph{Towards the Robustness of Differentially Private Federated Learning} \cite{OriginalPaper_1} challenges this belief, demonstrating that DP-FL is not inherently robust. In fact, standard outlier detection mechanisms (like Krum or Median) fail in DP-FL because the heavy DP noise increases the variance of benign updates, masking the malicious ones.\\

To exploit this, the authors propose \textbf{Attack-DPFL}. The core insight is that while benign updates are perturbed by noise with a large norm ($||n||_2 \approx \sigma\sqrt{d}$), malicious clients can upload unperturbed, "clean" poisoned gradients that are amplified to match this large norm.
\begin{equation}
    A_u = A \cdot G_u, \quad \text{where } A = \frac{||S_{benign}||}{||G_u||}
\end{equation}
By scaling the poisoned gradient $G_u$ by factor $A$, the attack hides the malicious update within the magnitude of the noise, allowing it to dominate the aggregation without being detected as an outlier.

To counter this, the paper introduces \textbf{Robust-DPFL}. The defense relies on the observation that DP noise vectors are independent and zero-mean, meaning their element-wise sum tends toward zero. In contrast, the amplified poisoned updates introduce a systematic bias. The defense calculates a \emph{Detection Score} $Z(S)$ for each update:
\begin{equation}
    Z(S) = \left| \frac{1}{d} \sum_{i=1}^{d} S[i] \right|
\end{equation}
Updates with high detection scores are identified as malicious and filtered out via K-means clustering.\\

The presenting group \cite{Presenters_1} reproduced the experiments on MNIST, FEMNIST, and CIFAR-10. Their evaluation confirmed that Attack-DPFL achieves an Attack Success Rate (ASR) of nearly 100\% against standard defenses like Krum and Median, which struggle to distinguish noise from poison. 

They further analyzed the "Three-way Trade-off" between Privacy, Accuracy, and Robustness using the Robust-DPFL defense. They observed that while Robust-DPFL successfully lowers ASR, increasing the privacy level (lowering the privacy budget $\epsilon$) introduces a side effect: the intense noise distorts gradient distributions so heavily that the score-based detection becomes slightly less precise. Despite this, they concluded that Robust-DPFL achieves the optimal balance, maintaining high main task accuracy where other defenses fail.

The presenters have outlined critical areas for future investigation, specifically emphasizing \textbf{Adaptive Privacy} strategies. While the current approach uses static noise levels, a dynamic mechanism that adjusts in real-time to the estimated threat severity could better preserve model accuracy. Additionally, the presenters highlight the risk of "Smarter Attackers" capable of circumventing the defense by optimizing their updates to yield a low Detection Score $Z(S)$. Countering such adversaries will necessitate the development of more sophisticated defense metrics that go beyond simple element-wise averaging.




\begin{thebibliography}{1}

    \bibitem{Presenters_1} 
    K. Ise, W. Tang and R. Yagi. Group Project: Towards the Robustness of Differentially Private Federated Learning. {\em Trustworthy Distributed Learning, TUM},
    Jan. 2026.

    \bibitem{OriginalPaper_1} 
    T. Qi, H. Wang and Y. Huang. Towards the Robustness of Differentially Private Federated Learning. {\em AAAI Conference on Artificial Intelligence (AAAI-24)},
    Vol. 38, No. 19, pp. 19911-19919, Mar. 2024.
\end{thebibliography}



\end{document}

@inproceedings{inproceedings,
author = {Xie, Yueqi and Fang, Minghong and Gong, Neil},
year = {2025},
month = {06},
pages = {1},
title = {Model Poisoning Attacks to Federated Learning via Multi-Round Consistency},
doi = {10.1109/CVPR52734.2025.01440}
}