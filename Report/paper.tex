% Version  Date        Author            Notes
% 1        ?           Tobias Lutz
% 2        April 2016  Tasnad Kernetzky  Updated to new IEEEtran class (v. 1.8)
%
%
\documentclass[journal, a4paper]{IEEEtran}

% check if we are running lua(la)tex and load font related packages appropriately
\usepackage{ifluatex}
\ifluatex
    \usepackage{fontspec}
\else
    \usepackage[T1]{fontenc}
    \usepackage[utf8]{inputenc}
\fi

% some very useful LaTeX packages include:
%\usepackage{cite}      % Written by Donald Arseneau
                        % V1.6 and later of IEEEtran pre-defines the format
                        % of the cite.sty package \cite{} output to follow
                        % that of IEEE. Loading the cite package will
                        % result in citation numbers being automatically
                        % sorted and properly "ranged". i.e.,
                        % [1], [9], [2], [7], [5], [6]
                        % (without using cite.sty)
                        % will become:
                        % [1], [2], [5]--[7], [9] (using cite.sty)
                        % cite.sty's \cite will automatically add leading
                        % space, if needed. Use cite.sty's noadjust option
                        % (cite.sty V3.8 and later) if you want to turn this
                        % off. cite.sty is already installed on most LaTeX
                        % systems. The latest version can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/cite/

\usepackage{graphicx}   % Written by David Carlisle and Sebastian Rahtz
                        % Required if you want graphics, photos, etc.
                        % graphicx.sty is already installed on most LaTeX
                        % systems. The latest version and documentation can
                        % be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/required/graphics/
                        % Another good source of documentation is "Using
                        % Imported Graphics in LaTeX2e" by Keith Reckdahl
                        % which can be found as esplatex.ps and epslatex.pdf
                        % at: http://www.ctan.org/tex-archive/info/

%\usepackage{psfrag}    % Written by Craig Barratt, Michael C. Grant,
                        % and David Carlisle
                        % This package allows you to substitute LaTeX
                        % commands for text in imported EPS graphic files.
                        % In this way, LaTeX symbols can be placed into
                        % graphics that have been generated by other
                        % applications. You must use latex->dvips->ps2pdf
                        % workflow (not direct pdf output from pdflatex) if
                        % you wish to use this capability because it works
                        % via some PostScript tricks. Alternatively, the
                        % graphics could be processed as separate files via
                        % psfrag and dvips, then converted to PDF for
                        % inclusion in the main file which uses pdflatex.
                        % Docs are in "The PSfrag System" by Michael C. Grant
                        % and David Carlisle. There is also some information
                        % about using psfrag in "Using Imported Graphics in
                        % LaTeX2e" by Keith Reckdahl which documents the
                        % graphicx package (see above). The psfrag package
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/psfrag/

%\usepackage{subfigure} % Written by Steven Douglas Cochran
                        % This package makes it easy to put subfigures
                        % in your figures. i.e., "figure 1a and 1b"
                        % Docs are in "Using Imported Graphics in LaTeX2e"
                        % by Keith Reckdahl which also documents the graphicx
                        % package (see above). subfigure.sty is already
                        % installed on most LaTeX systems. The latest version
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/subfigure/

%\usepackage{url}       % Written by Donald Arseneau
                        % Provides better support for handling and breaking
                        % URLs. url.sty is already installed on most LaTeX
                        % systems. The latest version can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/other/misc/
                        % Read the url.sty source comments for usage information.

%\usepackage{stfloats}  % Written by Sigitas Tolusis
                        % Gives LaTeX2e the ability to do double column
                        % floats at the bottom of the page as well as the top.
                        % (e.g., "\begin{figure*}[!b]" is not normally
                        % possible in LaTeX2e). This is an invasive package
                        % which rewrites many portions of the LaTeX2e output
                        % routines. It may not work with other packages that
                        % modify the LaTeX2e output routine and/or with other
                        % versions of LaTeX. The latest version and
                        % documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/sttools/
                        % Documentation is contained in the stfloats.sty
                        % comments as well as in the presfull.pdf file.
                        % Do not use the stfloats baselinefloat ability as
                        % IEEE does not allow \baselineskip to stretch.
                        % Authors submitting work to the IEEE should note
                        % that IEEE rarely uses double column equations and
                        % that authors should try to avoid such use.
                        % Do not be tempted to use the cuted.sty or
                        % midfloat.sty package (by the same author) as IEEE
                        % does not format its papers in such ways.

\usepackage{amsmath}   % From the American Mathematical Society
                        % A popular package that provides many helpful commands
                        % for dealing with mathematics. Note that the AMSmath
                        % package sets \interdisplaylinepenalty to 10000 thus
                        % preventing page breaks from occurring within multiline
                        % equations. Use:
\interdisplaylinepenalty=2500
                        % after loading amsmath to restore such page breaks
                        % as IEEEtran.cls normally does. amsmath.sty is already
                        % installed on most LaTeX systems. The latest version
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/



% Other popular packages for formatting tables and equations include:

%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty which improves the
% LaTeX2e array and tabular environments to provide better appearances and
% additional user controls. array.sty is already installed on most systems.
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/

% V1.6 of IEEEtran contains the IEEEeqnarray family of commands that can
% be used to generate multiline equations as well as matrices, tables, etc.

% Also of notable interest:
% Scott Pakin's eqparbox package for creating (automatically sized) equal
% width boxes. Available:
% http://www.ctan.org/tex-archive/macros/latex/contrib/supported/eqparbox/

% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.

\usepackage{algorithm}
\usepackage{algpseudocode} 
% Your document starts here!
\begin{document}

% Define document title and author
\title{{Report on}\\ \Large LeadFL: Client Self-Defense against Model Poisoning in Federated Learning}
\author{\textbf{Claus Guthmann}, Ramazan Tan}

\markboth{Trustworthy Distributed Learning }{}
\maketitle

%
% LeadFL - 4 Pages
%

% Write abstract here(50-80 words)
\begin{abstract}
    Existing defense mechanisms in federated learning often assume an approximately constant proportion of 
    malicious clients throughout training. However, when only a subset of clients participates in each 
    communication round, the fraction of malicious clients can vary substantially across rounds. 
    Over a sufficient number of iterations, this variability makes it likely that some rounds will 
    contain a majority of malicious participants. Under this premise, the paper \textit{LeadFL: Client Self-Defense against Model Poisoning in Federated Learning} \cite{LeadFL} demonstrates that poisoning attacks occurring in such rounds can have long-lasting effects on the global model. To address this issue, the authors propose a client-side defense mechanism that significantly improves the model’s ability to recover from these transient but severe poisoning events.

\end{abstract}

\section{Introduction}
\IEEEPARstart{f}{ederated} 
 learning enables the collaborative training of machine learning models across distributed clients while keeping local data private. However, this decentralized setting also exposes the training process to model poisoning attacks, where malicious clients manipulate their updates to degrade the global model. Many existing defense mechanisms implicitly assume that the proportion of malicious clients remains approximately constant throughout training. In practice, federated learning systems typically select only a subset of clients to participate in each communication round, causing the fraction of malicious clients to fluctuate over time. As a result, it becomes increasingly likely that some rounds contain an unusually high—or even majority—share of malicious participants, especially over long training horizons.

The paper \textit{LeadFL: Client Self-Defense against Model Poisoning in Federated Learning} \cite{LeadFL} addresses this often-overlooked scenario. It shows that poisoning attacks occurring during such adversarial spikes can have long-lasting effects on the global model, even when the system later returns to benign conditions. To mitigate this issue, LeadFL introduces a client-side defense mechanism designed to limit the impact of these transient but severe attacks and to accelerate recovery in subsequent rounds.

In this report, we begin by presenting the preliminary concepts and results required to understand the LeadFL approach. We then provide an overview of the method itself and discuss its potential advantages in federated learning environments subject to spiking adversarial participation, followed by an examination of its limitations. To further assess its effectiveness, we implemented the LeadFL algorithm and extended the original evaluation to cover previously unaddressed cases. Finally, we present the results of our own experiments, which corroborate that LeadFL significantly improves robustness against the attack patterns considered in the paper.

\section{Background}
A central idea is to reduce the lasting influence of a burstial attack. One way to formally characterize this lingering effect was introduced in \textit{FL-WBC: Enhancing Robustness against Model Poisoning Attacks in Federated Learning from a Client Perspective} \cite{FLwbc} through the notion of the \textbf{Attack Effect on Parameters}. For a given communication round $t$, let $W_t$ denote the global model parameters resulting from potentially malicious client updates, and let $W_t^M$ denote the parameters that would have been obtained in the absence of malicious behavior. The attack effect is then defined as
\begin{equation}
\delta_t := W_t - W_t^M.    
\end{equation}

The authors further derive a recursive approximation of this quantity,
\[
\delta_t \approx \hat{\delta}_t := \frac{1}{|C_t|} \left[ \sum_{c \in C_t} \prod_{i=0}^{I-1} \left( I - \eta_t H_{t,i}^c \right) \right] \hat{\delta}_{t-1},
\]
where $C_t$ denotes the set of clients selected in round $t$ (assumed to be equally weighted), $\eta_t$ is the learning rate, $I$ is the number of local training steps, and $H_{t,i}^c$ represents the Hessian matrix of the local loss function.

Empirically, the Hessian matrices $H_{t,i}^c$ are often highly sparse, which causes the aggregated transformation
\begin{equation}
\frac{1}{|C_t|} \left[ \sum_{c \in C_t} \prod_{i=0}^{I-1} \left( I - \eta_t H_{t,i}^c \right) \right]
\end{equation}
to remain close to the identity matrix. As a consequence, the attack effect $\delta_t$ decays only slowly over time, leading to the observed persistence of poisoning attacks.

The FL-WBC \cite{FLwbc} client-side defense seeks to counteract this  by injecting random noise into local updates, thereby accelerating the decay of the attack effect. Building on this insight, LeadFL \cite{LeadFL} proposes a more targeted approach: instead of relying on random perturbations, it introduces a specifically designed regularization term.

Directly computing the full Hessian matrix $H_{t,i}^c$ for  neural networks is, however, computationally prohibitive. To make the approach tractable in a federated learning setting, LeadFL relies on an efficient approximation of the Hessian inspired by the \textit{Optimal Brain Damage} framework \cite{OPD}.

Specifically, the method restricts attention to the diagonal elements of the Hessian, thereby ignoring inter-parameter dependencies. While this simplification sacrifices second-order interactions, it significantly reduces computational complexity and has been shown to provide useful curvature information in large-scale models. The diagonal entries capture the sensitivity of the gradient with respect to individual parameters and can be approximated via finite differences of successive gradients.

Formally, the Hessian matrix is approximated as
\begin{equation}
    \label{equation:diag}
    H_{t,i}^c \approx \tilde H_{t,i}^c := \operatorname{diag}\bigl(\nabla L(W_{t,i+1}^c) - \nabla L(W_{t,i}^c)\bigr),
\end{equation}
where $W_{t,i}^c$ denotes the local model parameters of client $c$ at global round $t$ and local step $i$, and $L(\cdot)$ is the local loss function. 

\section{LeadFL}
The core idea of LeadFL \cite{LeadFL} is to reduce the persistence of poisoning effects by locally minimizing the term $I - H_{t,i}^c$ on each client, thereby accelerating the decay of the attack effect described in the previous section. To achieve this, LeadFL augments the standard local optimization objective with an additional regularization term whose gradient is incorporated into each update step.

Concretely, each local update is computed in two stages. First, a standard gradient descent step is performed,
\begin{equation}
    \tilde W_{t,i+1}^c := W_{t,i}^c - \eta_t \nabla L(W_{t,i}^c),
\end{equation}
yielding intermediate model parameters $\tilde W_{t,i+1}^c$. In a second step, the regularization update is applied,
\begin{equation}
    W_{t,i+1}^c := \tilde W_{t,i+1}^c - \eta_t \alpha \, \operatorname{clip}\!\left( \nabla \bigl( I - \eta_t \tilde H_{t,i}^c \bigr), q \right),
\end{equation}
where $\alpha$ controls the strength of the regularization and \emph{clip} limits the absolute value of each entry to the clipping threshold $q$.

This two-stage formulation allows the intermediate parameters $\tilde W_{t,i+1}^c$ to be reused in constructing a computationally efficient approximation of the Hessian. However, the derivation of this approximation in the original paper contains errors. We therefore adopt the following formulation, which we believe to be a mathematically consistent approximation closely aligned with the intended method:
\begin{equation}
    \tilde H_{t,i}^c \approx (W_{t,i}^c - \tilde W_{t,i+1}^c + \Delta W_{t,i}^c)/\eta_t,
\end{equation}
where $\Delta W_{t,i}^c := W_{t,i}^c - W_{t,i-1}^c$ denotes the parameter change between successive local steps. It is important to note that this Hessian approximation is only accurate up to the contribution of the regularization term applied in the previous local update. 

Combining the previous steps, the local client iteration of LeadFL can be summarized as follows (cf. Algorithm 1 in \cite{LeadFL}):

\begin{algorithm}
\caption{\emph{LeadFL} Local Client Iteration}
\begin{enumerate}[]
    \item[1:] {Compute gradients and update intermediate weights:} \\
    $\tilde W_{t,i+1}^c \leftarrow W_{t,i}^c - \eta_t \nabla L(W_{t,i}^c)$
    
    \item[2:] {Estimate the Hessian matrix:} \\
    $\tilde H_{t,i}^c \leftarrow (W_{t,i}^c - \tilde W_{t,i+1}^c + \Delta W_{t,i}^c)/\eta_t$
    
    \item[3:] {Compute the regularization term:} \\
    $R_{t,i}^c \leftarrow \operatorname{clip}\!\left( \nabla \bigl(I - \eta_t \tilde H_{t,i}^c \bigr), q \right)$
    
    \item[4:] {Update the local weights with the regularization term:} \\
    $W_{t,i+1}^c \leftarrow \tilde W_{t,i+1}^c - \eta_t \alpha R_{t,i}^c$
\end{enumerate}
\end{algorithm}



The authors of LeadFL subsequently implement and evaluate the proposed defense under a range of experimental settings. Their primary setup consists of 100 clients, of which 25 are malicious. Training is performed over 80 global rounds, with each client executing 10 local training steps per round. In each global round, 10 clients are selected uniformly at random; this client selection is kept consistent across different experimental runs to ensure comparability (additional results for alternative client selection strategies are reported in the appendix of \cite{LeadFL}).

The evaluation considers several server-side aggregation defenses, namely SparseFed, Multi-Krum, and Bulyan. For the LeadFL client-side defense, the clipping threshold is fixed at $q = 0.2$. The regularization strength $\alpha$ is varied across datasets, with $\alpha = 0.4$ for FashionMNIST and $\alpha = 0.25$ for CIFAR10. Due to the randomized client selection process, the number of malicious participants fluctuates across rounds, leading to significant variability in backdoor success rates. To account for this effect, the authors report both the final backdoor accuracy and the average backdoor accuracy over the entire training process.

\begin{table}[!hbt]
    \begin{center}
    \caption{Comparison of defenses under 9-pixel pattern backdoor attack on IID FashionMNIST dataset. ( cf. Table 1 \cite{LeadFL})}
    \label{tab:IIDMNIST}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Server-side Defense & \multicolumn{4}{|c|}{Multi-Krum} \\\hline 
        Client-side Defense & None & LDP & FL-WBC & LeadFL \\ \hline 
        Maintask Accuracy  & 89.3 & 87 & 87.2 & 87.9  \\ \hline 
        Backdoor Accuracy Avg.  & 82.6 & 76.0 & 77.5 & 32.9 \\ \hline 
        Backdoor Accuracy Final  &93.2  &79.6  &80.6 &0.0 \\ \hline 
    \end{tabular}
    \end{center}
\end{table}
\begin{table}[!hbt]
    \begin{center}
    \caption{Comparison of defenses under 9-pixel pattern backdoor attack on IID CIFAR10 dataset. ( cf. Table 2 \cite{LeadFL})}
    \label{tab:IIDCIFAR}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Server-side Defense & \multicolumn{4}{|c|}{Multi-Krum} \\\hline 
        Client-side Defense & None & LDP & FL-WBC & LeadFL \\ \hline 
        Maintask Accuracy  & 76.3 &48.0 &43.3 &56.9  \\ \hline 
        Backdoor Accuracy Avg.  & 77.5 &53.1& 56.9 &35.6 \\ \hline 
        Backdoor Accuracy Final  &80.5 &43.8& 40.5 &25.6 \\ \hline 
    \end{tabular}
    \end{center}
\end{table}

Tables~\ref{tab:IIDMNIST} and~\ref{tab:IIDCIFAR} present a selection of results for the IID FashionMNIST and CIFAR10 datasets, respectively, under a 9-pixel pattern backdoor attack using the Multi-Krum server-side defense.

For the FashionMNIST dataset (Table~\ref{tab:IIDMNIST}), LeadFL consistently reduces the final backdoor accuracy to nearly zero while maintaining the main task accuracy within a 10\% reduction compared to the undefended baseline. In contrast, the alternative client-side defenses, LDP and FL-WBC, achieve only moderate reductions in backdoor accuracy.

For the more complex CIFAR10 dataset (Table~\ref{tab:IIDCIFAR}), the impact of LeadFL on the main task accuracy is more pronounced, reducing it to approximately 50--60\%. Nevertheless, LeadFL still substantially lowers the final backdoor accuracy to around 25\%, outperforming the other client-side defenses in terms of robustness.

Across both datasets and all evaluated server-side defenses, the absence of a client-side defense results in consistently high average backdoor accuracies—exceeding 75\%—highlighting the severity of bursty poisoning attacks and the necessity of additional client-side protection mechanisms.

\begin{figure}[!hbt]
    \begin{center}
        \caption{9-pixel back door attack on FashionMNIST when the attack is periodic(cf. Figure 3 \cite{LeadFL})}
    \label{fig:MNIST}
    \includegraphics[width=0.4\textwidth]{figs/MNIST-IID.png}
    \end{center}
\end{figure}

\begin{figure}[!hbt]
    \begin{center}
        \caption{9-pixel back door attack on CIFAR10 when the attack is periodic (cf. Figure 3 \cite{LeadFL})}
    \label{fig:CIFAR}
    \includegraphics[width=0.4\textwidth]{figs/CIFAR10IID.png}
    \end{center}
\end{figure}

The authors further investigate the behavior of LeadFL under explicitly bursty attack patterns. In this setting, client selection is no longer random. Instead, training proceeds in cycles of ten rounds, where the first two rounds contain six malicious clients out of ten selected participants, while the remaining eight rounds involve exclusively benign clients. 

The results of this experiment are illustrated in Figure~\ref{fig:MNIST} for the FashionMNIST dataset and in Figure~\ref{fig:CIFAR} for the CIFAR10 dataset. While LeadFL exhibits limited impact during the rounds dominated by malicious clients, its effect becomes apparent in the subsequent benign rounds. In particular, the recovery from each attack spike is significantly faster, and the achieved backdoor accuracy stabilizes at a lower level compared to the undefended baseline.

In summary, LeadFL introduces a novel client-side defense mechanism that is particularly effective at mitigating the long-lasting effects of spikes in malicious client participation. Although the method incurs a measurable reduction in main task accuracy, it demonstrates that client-side defenses can play a crucial role in complementing existing server-side aggregation strategies. These findings motivate a closer examination of LeadFL's practical limitations and empirical behavior, which we address in the remainder of this report through an extended experimental evaluation.


\section{Limitations and Extended Evaluation}
Before presenting our own experimental evaluation, we first discuss several limitations and ambiguities encountered in the original LeadFL paper. These issues are relevant both for the correct interpretation of the reported results and for the reproducibility of the proposed method.

First, as discussed in the previous sections, the Hessian approximation used in LeadFL is not derived rigorously and appears to contain minor mathematical inconsistencies. As a result, any practical implementation—including our own—necessarily relies on an approximation that deviates from the formulation presented in the paper. 

Second, we observed discrepancies between the algorithmic description in the paper and the behavior required for a functioning implementation. 

Finally, while the regularization strength $\alpha$ plays a crucial role in balancing robustness against performance degradation, the paper does not provide a principled justification or systematic tuning strategy for the selected values. Instead, $\alpha$ is varied empirically across experiments without clear guidance, limiting the interpretability and generalizability of the reported outcomes.


% Now we need a bibliography:
\begin{thebibliography}{1}

    
    \bibitem{LeadFL} 
    C.~Zhu, S.~Roos, and L.~Chen. LeadFL: Client Self-Defense against Model Poisoning in Federated Learning. {\em Proceedings of the 40th International Conference on Machine Learning},
    vol.~202, pp.~43158--43180, Jul. 2023.
    
    \bibitem{FLwbc} 
    J.~Sun, A.~Li, L.~DiValentin, A.~Hassanzadeh, Y.~Chen and H.~Li. {FL-WBC:} Enhancing Robustness against Model Poisoning Attacks in
                  Federated Learning from a Client Perspective {\em Advances in Neural Information Processing Systems},
    vol.~34, pp.~12613--12624, Dec. 2021.

    \bibitem{OPD} 
    Y.~LeCun, J.~Denker and S.~Solla. Optimal Brain Damage {\em Advances in Neural Information Processing Systems},
    vol.~2, pp.~598-605, Nov. 1989.

\end{thebibliography}


\section{Summary: }

\end{document}
